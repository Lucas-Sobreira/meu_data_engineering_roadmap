from airflow.decorators import dag, task
from airflow.operators.python import BranchPythonOperator
from src.parquet_s3 import verifica_arquivos_source, load_settings, configurando_ambiente, arquivos_output, upload_parquet_s3, delete_source_files
from datetime import datetime

@dag(
    dag_id="pipeline_s3_parquet",
    description="Envio de arquivos locais no formato parquet para o Bucket S3 da AWS.",
    schedule="0 * * * *",  # CRON ajustado para rodar diariamente
    start_date=datetime(2025, 2, 24),
    catchup=False
)
def pipeline_parquet_s3():
    """
    1. Verifica se hÃ¡ arquivos na pasta 'source';
    2. Se houver arquivos, inicia o pipeline;
    3. Caso contrÃ¡rio, encerra a execuÃ§Ã£o e aguarda a prÃ³xima tentativa.

    Fluxo do Pipeline:
        1. LÃª variÃ¡veis de ambiente do .env;
        2. Configura ambiente e diretÃ³rios;
        3. Gera e salva arquivos Parquet na pasta 'output';
        4. Faz upload dos arquivos para o S3;
        5. Remove os arquivos originais da pasta 'source'.
    """

    def decide_proximo_passo():
        """ Decide se o DAG deve continuar ou encerrar """
        if verifica_arquivos_source():
            return "load_settings"  # Continua o fluxo normalmente
        else:
            return "fim_processo"   # Interrompe o DAG sem erro

    verifica_arquivos = BranchPythonOperator(
        task_id="verifica_arquivos_source",
        python_callable=decide_proximo_passo
    )

    @task(task_id='fim_processo')
    def task_fim_processo():
        """ Tarefa vazia para encerrar o DAG se nÃ£o houver arquivos """
        print("ðŸš« Nenhum arquivo encontrado. Encerrando DAG.")

    @task(task_id='load_settings')
    def task_load_settings():
        print("ðŸ” Lendo variÃ¡veis do '.env'")
        return load_settings()

    @task(task_id='configurando_ambiente')
    def task_configurando_ambiente():
        print("ðŸ› ï¸ Configurando ambiente e diretÃ³rios...")
        return configurando_ambiente()
    
    @task(task_id='lista_arquivos_destino')
    def task_arquivos_destino(path_list: list):
        print("ðŸ“‚ Criando arquivos Parquet na pasta 'Output'")
        return arquivos_output(path_list)

    @task(task_id='upload_parquet_s3')
    def task_upload_parquet_s3(settings: dict):
        print("ðŸ“¤ Enviando arquivos Parquet para o S3...")
        return upload_parquet_s3(settings)
    
    @task(task_id='delete_source_files')
    def task_delete_source_files(path_list: list):
        print("ðŸ—‘ï¸ Removendo arquivos originais da pasta 'Source'")
        return delete_source_files(path_list)

    # Caso nÃ£o contenha arquivos irÃ¡ esperar a prÃ³xima "rodada"
    fim = task_fim_processo()

    # DefiniÃ§Ã£o do fluxo de execuÃ§Ã£o    
    t1 = task_load_settings()               # Carrega variÃ¡veis do .env
    t2 = task_configurando_ambiente()       # Configura ambiente
    t3 = task_arquivos_destino(t2)          # Gera arquivos na pasta output
    t4 = task_upload_parquet_s3(t1)         # Upload para S3 (depende de t1 e t3)
    t5 = task_delete_source_files(t2)       # Remove arquivos da source (depende de t2 e t3)

    # LÃ³gica de Branching
    verifica_arquivos >> [t1, fim]  # Se houver arquivos, continua para t1; senÃ£o, encerra

    # Encadeamento correto das tarefas
    t1 >> t2 >> t3
    t3 >> [t4, t5]  # Upload e delete acontecem em paralelo

pipeline_parquet_s3()